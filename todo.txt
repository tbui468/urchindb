Look here for hints on creating a database:
    https://cs.brown.edu/courses/csci1270/website_2021/

should write test that checks data persistence between multiple opens/closes

Paging - NOTE: reset BUCKETS_MAX back to 1024 before running tests for cache buffers
    Test results without paging with 1024 buckets (bucket count didn't seem to make a noticeable difference)
        seems like file access was the bottleneck, and not traversing the hash chains.
        n = 500:  10.15s
        n = 1000: 39.54s
        n = 2000: 156.34

    Cache buffer - single process without any multiprocessor protection
        n = 500:  0.29s
        n = 1000: 0.91s
        n = 2000: 2.66s

    Cache buffer - multiprocess protection / locking / copy-on-write / checking timestamps for stagnant data
        Should be slower than above, but hopefully a fair bit faster than direct file access.  Make the first
        8 bytes hold a uint64_t unix time - whenever reading/writing, put a write lock/read lock on these 8 bytes
        Timestamp is NOT fine-grain enough - it's in seconds, but a lot can happen in a second
        Could use update count for now - use it to track updates.

        Single block
            n = 500:  0.30s
            n = 1000: 0.89s
            n = 2000: 2.53s

        4096 byte blocks

Implement key/value database

    Index Record:
        uint32_t next_off
        uint32_t data_off
        uint32_t data_size
        uint32_t key_size
        char* key

    Data Record
        char* data

    enum DbType {
        DB_INT,
        DB_STRING
    };

    struct Field {
        char* name;
        enum DbType type;
    };

    struct Field fields[3];
    fields[0].name = "first_name";
    fields[0].type = DB_STRING;
    fields[0].name = "last_name";
    fields[0].type = DB_STRING;
    fields[0].name = "age";
    fields[0].type = DB_INT;
    db_create("Students", fields, 3);

    Questions:
        How are data files and index files stored now?
            data should be stored with fields sequentially: primary key, and then fields in order
                DB_STRING start with an uint32_t specifying length of string, followed by string
                DB_INT is just a 4 byte integer for now
            index file initially create with primary key as index, stored in tree (just use binary tree for simplicity now)
                each node has the primary key, and also the offset into the data file
                indexing by a different field will create a new index file
                    students.idx.primary_key - this needs to be disallowed as a field name
                    students.idx.age
                    students.dat

    When opening a database, must define a schema for data
        db_open() <-----only works with database that already exists
        db_create() <-----need to be called before db_open can be used, fails if database with given name already exists
        db_create("students", char** fields, enum db_types* types);  <---returns 0 if database was succesfully create
            user still needs to call db_open to open the newly created database
            how about indexing tables?  Default is primary key (uint32_t)
            db_index(db, char* field) <---create new index using this field in table

        B tree instead of hash table Why use a b tree?  If data is index using a b-tree, we can search for ALL records that match a condition
                eg, if age > 0 and age < 10.
                Right now, we can't do that
            But would require user defining a table
                database would assign primary keys and index by primary keys by default
                when making table, user could specify which field to index

    db_insert
    db_update
    db_delete
    db_select(db, "age", age_fun); //select all records where age field is passed to age_fun, and result is true

    SQL and engine to search database

    Apply fine-grain locking
        Where should the lock be applied?  (eg, what byte region)
            Lock the entire chain - but other chains (including the freelist) can still be used by other processes

    Does paging make database faster?
        Write some tests that write/read to database.
        Add pager that reads fixed data from the index and data files into memory
        Unless data is written, just read data from buffers in memory instead of accessing disk
        Only when something is written (eg, using db_store), do we write buffers back to disk

    Make it a distributed database - need to add network support
        how to keep all databases consistent?  !!!consensus!!!
        Should all instances have the same internal format, or just the same keys/values?
        Would also need to timestamp stuff
