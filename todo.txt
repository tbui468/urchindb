Look here for hints on creating a database:
    https://cs.brown.edu/courses/csci1270/website_2021/

Paging - NOTE: reset BUCKETS_MAX back to 1024 before running tests for cache buffers
    Test results without paging with 1024 buckets (bucket count didn't seem to make a noticeable difference)
        seems like file access was the bottleneck, and not traversing the hash chains.
        n = 500:  10.15s
        n = 1000: 39.54s
        n = 2000: 156.34

    Cache buffer - single process without any multiprocessor protection
        n = 500:  0.29s
        n = 1000: 0.91s
        n = 2000: 2.66s

    Cache buffer - multiprocess protection / locking / copy-on-write / checking timestamps for stagnant data
        Should be slower than above, but hopefully a fair bit faster than direct file access

    _db_new_idxrec is returning the same offset when storing two different records

    Could just write to buffer for now to see what happens - just forget about file locking and multiprocesses for now
        If it's fairly faster, then we know for certain that file access is the bottleneck.  Multiprocess stuff can
        be figured out later.

    What about processes that have stale data when reading?  How would other processes know
        if new data was written to database???  This wouldn't be possible unless the process
        STILL read the file for a timestamp to check if it was updated...that would defeat
        the goal of NOT reading the file everytime a query was sent by a process.
    
        Could have the first n bytes of a file be a timestamp
            Everytime a query is sent, process must check timestamp of last write
            to make sure that data is stagnant.  If so, must reload (all for now)
            data into buffer before reading.  A read lock can be used on the file then.

            On writing data, also must perform a write lock.  This solves the problem, assuming
            that the number of bytes read from a file is proportional to the time it takes
            eg, reading 10 bytes takes twice as long as 5 bytes.  But would copying all of
            the file into the buffers be very inefficient???  

            Could reserve first dozen bytes for timestamps for each block - then only
            copy blocks where the timestamp is invalid.  But would need to do the same for
            both files.  And what if the files expand to a larger size?  Would need to rewrite
            so that enough room is reserved at the beginning of the file.

    Is there an easier way to cache data from the file?

    Need to write data into buffers instead of disk for now
        When opening a new database:
            if it already exists, open and fill both buffers
            else, open, and then fill index buffer with default buckets and free list
                will leave writing to files when database is closed

        replace ALL _fread, _fwrite, _fseek, and _ftell with:
            _db_read, _db_write, _db_seek, _db_tell (_db_tell could just return buffer size for now)
    
    Then, when db_close is called, write buffers into disk once

    Rerun tests to see if any improvement (should be quite significant)

    Will need copy-on-write capabilities (including locking of files)
        How can this be tested?  Using same testing code as before to test locking files?

    Should be able to read/copy-on-write only segments of the file instead of entire thing

Implement key/value database

    Index Record:
        uint32_t next_off
        uint32_t data_off
        uint32_t data_size
        uint32_t key_size
        char* key

    Data Record
        char* data

    enum DbType {
        DB_INT,
        DB_STRING
    };

    struct Field {
        char* name;
        enum DbType type;
    };

    struct Field fields[3];
    fields[0].name = "first_name";
    fields[0].type = DB_STRING;
    fields[0].name = "last_name";
    fields[0].type = DB_STRING;
    fields[0].name = "age";
    fields[0].type = DB_INT;
    db_create("Students", fields, 3);

    Questions:
        How are data files and index files stored now?
            data should be stored with fields sequentially: primary key, and then fields in order
                DB_STRING start with an uint32_t specifying length of string, followed by string
                DB_INT is just a 4 byte integer for now
            index file initially create with primary key as index, stored in tree (just use binary tree for simplicity now)
                each node has the primary key, and also the offset into the data file
                indexing by a different field will create a new index file
                    students.idx.primary_key - this needs to be disallowed as a field name
                    students.idx.age
                    students.dat

    When opening a database, must define a schema for data
        db_open() <-----only works with database that already exists
        db_create() <-----need to be called before db_open can be used, fails if database with given name already exists
        db_create("students", char** fields, enum db_types* types);  <---returns 0 if database was succesfully create
            user still needs to call db_open to open the newly created database
            how about indexing tables?  Default is primary key (uint32_t)
            db_index(db, char* field) <---create new index using this field in table

        B tree instead of hash table Why use a b tree?  If data is index using a b-tree, we can search for ALL records that match a condition
                eg, if age > 0 and age < 10.
                Right now, we can't do that
            But would require user defining a table
                database would assign primary keys and index by primary keys by default
                when making table, user could specify which field to index

    db_insert
    db_update
    db_delete
    db_select(db, "age", age_fun); //select all records where age field is passed to age_fun, and result is true

    SQL and engine to search database

    Apply fine-grain locking
        Where should the lock be applied?  (eg, what byte region)
            Lock the entire chain - but other chains (including the freelist) can still be used by other processes

    Does paging make database faster?
        Write some tests that write/read to database.
        Add pager that reads fixed data from the index and data files into memory
        Unless data is written, just read data from buffers in memory instead of accessing disk
        Only when something is written (eg, using db_store), do we write buffers back to disk

    Make it a distributed database - need to add network support
        how to keep all databases consistent?  !!!consensus!!!
        Should all instances have the same internal format, or just the same keys/values?
        Would also need to timestamp stuff
