Look here for hints on creating a database:
    https://cs.brown.edu/courses/csci1270/website_2021/

Andy Pavlo lectures from Carnegie Mellon University are really good!!!
    https://www.youtube.com/@CMUDatabaseGroup

1 Relational Model and Relational Algebra
    What is the difference between a data model and schema?
        A model includes relational, key/value, NoSQL, etc
        schema is the actual attributes and data types of a given relation
    What does DML stand for?
        Data manipulation language
    What is the difference between a set and a bag?
        sets don't contain duplicates, and bags do (or at least can)
    Explain each basic relational algebra operator (what SQL command corresponds to it):
        select
        projection
        union
        intersect
        difference
        product
        join
    Are there more relational algebra operators in modern databases?
        Yes!  Those 7 are just the most common/primitive operators
TODO:
    run metrics to see how much slower it is now

    Refactor files:
        urchindb.c
        table.c
            these functions require a massive refactor to remove repeated code/make it more readable
        utility.c
        pager.c
        main.c

    Make linked list of blocks doubling linked
    start block search from most recently access block instead of least recently used

Table sees data in contiguous memory - pager does all the loading/dropping of blocks/setting flags for dirty blocks

Once the block system works, rewrite so that we separate data and paging:
    urchin_db.c (public functions for client code)
    Back-end
        table.c (hash table - change this to tree.c later when changing to relational database)
        pager.c (this includes the interface between the pager and OS)
        logger.c (write-ahead logging or shadowing)
        osi.c (interface to allow usage with linux/macos/windows)
    utility.c (wrappers and error handling)
    Switch to tree.c format for row-relational database

    Front-end
        parser.c (parse SQL queries)
        generator.c (generate machine code to run on vm)
        vm.c (run machine code to manipulate data tree)

Or try to make the size/time relationship linear (quadratic now) or even logarithmic
    One option is only copying dirty blocks instead of entire file into buffer
    make Block 0 (first page) the superblock that holds metadata (including dirty bits)
    whenever a different block is dirty, update a bit so that we know when to copy/write to disk
    All reads/writes will check bits and reload buffer cache from disk if any dirty bits found

        Make first BLOCK_SIZE metadata
        When calling _calloc, make sure to include include meta data block

        Right now, we need to read the entire file into memory - this is not possible if file too big
            need to only load in parts of file - need to decide on eviction policy to determine which
            blocks stay in memory.

    Right now, we are reading in update count for the entire file/buffer.  Need to separate that in reading
        individual blocks.

        Might be time to separate into multiple translation units with bettered defined interfaces:
            cli.c - this is the public interface other programs/users have access to
                db_create - will add this later to allow table creation
                db_open
                db_store
                db_fetch
                db_rewind - will remove later
                db_nextrec - will remove later
                db_query(condition); - will replace with SQL engine later
            cache.c (either hash table or a tree representation)
                cache_new_idxrec
                cache_insert_idxrec
                cache_remove_idxrec
                data representation in buffer cache (either tree or hash table)
                includes finding new index records (either allocating a new one, or reusing one from the freelist)
                writing and reading data from the buffer cache
                writes dirty bits to allow pager to know which blocks to write/update from disk
            pager.c (this will take care of the OS interface for now)
                pager_reload_cache()
                pager_write_cache()
                pager_

Support multi-threading
    Write tests that show multi-threading fails (use pthreads)
    Implement locks to pass tests

Could make two front-ends, one for SQL and the other for QUEL.

Frontend - command line interface, parser, generator, vm
Backend - tree, pager, and OS interface
    cli.c
    tree.c
    pager.c
    osi.c

Schemas <----------start here with creating db_create()
    db_open() can only be called on databases that already exist
    User must call db_create(const char* name, char** field_names, enum DbTypes* field_types) to create a schema first <---------implement this function
    db_insert() //create new record
    db_update() //update existing record
    db_delete() //delete exiting record
    db_select(char* field_name, bool (*cmp)(void)); //selects all records where field satifies cmp function (eg, returns true)
        will replace with SQL engine later.  This function is just a temporary solution to test fetching data
        what kind of bytecode will the vm use?

Trees
    Let's say we use a binary tree to store records index by primary key
        Would need to rebalance tree as more records are added to make sure it's balanced
        otherwise a binary tree is pointless.  Also useful to rebalance trees if too high
        for the number of nodes

        How should the binary tree be stored?
            after update_count and free list root, store the root node
            Reading an index file will convert data to a binary tree
            Writing a binary tree to file will do just that

            Index by primary key
                primary key
                offset_count = 1
                record offset in data file (primary keys are unique, so just one offset)

            Index by age
                age
                offset_count = ???
                record offset in data file (may not be unique, so could be more than one offset)

        Need a function to rebalance tree after a few adds
            traverse tree and find lowest and highest values
            pick median value as the root
            rebalance tree based off that new root

Paging - NOTE: reset BUCKETS_MAX back to 1024 before running tests for cache buffers
    Test results without paging with 1024 buckets (bucket count didn't seem to make a noticeable difference)
        seems like file access was the bottleneck, and not traversing the hash chains.
        n = 500:  10.15s
        n = 1000: 39.54s
        n = 2000: 156.34

    Buffer Cache - single process without any multiprocessor protection
        n = 500:  0.29s
        n = 1000: 0.91s
        n = 2000: 2.66s

    Buffer Cache - multiprocess protection / file locking / copy-on-write / update stale data on read

        Single block with 1024 buckets
            n = 8000:  12.71s
            n = 16000: 46.94s
            n = 32000: 189.21s

        Loading in/out blocks based on least-recently used eviction policy
        Need this since database may be too big to hold entirely in memory
            n = 2000: 1.97s
            n = 4000: 4.30s
            n = 8000: 12.28s
            n = 16000: 35.65s
            n = 32000: 91.08s

    Multithreading
    

Implement key/value database

    Index Record:
        uint32_t next_off
        uint32_t data_off
        uint32_t data_size
        uint32_t key_size
        char* key

    Data Record
        char* data

    enum DbType {
        DB_INT,
        DB_STRING
    };

    struct Field {
        char* name;
        enum DbType type;
    };

    struct Field fields[3];
    fields[0].name = "first_name";
    fields[0].type = DB_STRING;
    fields[0].name = "last_name";
    fields[0].type = DB_STRING;
    fields[0].name = "age";
    fields[0].type = DB_INT;
    db_create("Students", fields, 3);

    Questions:
        How are data files and index files stored now?
            data should be stored with fields sequentially: primary key, and then fields in order
                DB_STRING start with an uint32_t specifying length of string, followed by string
                DB_INT is just a 4 byte integer for now
            index file initially create with primary key as index, stored in tree (just use binary tree for simplicity now)
                each node has the primary key, and also the offset into the data file
                indexing by a different field will create a new index file
                    students.idx.primary_key - this needs to be disallowed as a field name
                    students.idx.age
                    students.dat

    When opening a database, must define a schema for data
        db_open() <-----only works with database that already exists
        db_create() <-----need to be called before db_open can be used, fails if database with given name already exists
        db_create("students", char** fields, enum db_types* types);  <---returns 0 if database was succesfully create
            user still needs to call db_open to open the newly created database
            how about indexing tables?  Default is primary key (uint32_t)
            db_index(db, char* field) <---create new index using this field in table

        B tree instead of hash table Why use a b tree?  If data is index using a b-tree, we can search for ALL records that match a condition
                eg, if age > 0 and age < 10.
                Right now, we can't do that
            But would require user defining a table
                database would assign primary keys and index by primary keys by default
                when making table, user could specify which field to index

    db_insert
    db_update
    db_delete
    db_select(db, "age", age_fun); //select all records where age field is passed to age_fun, and result is true

    SQL and engine to search database

    Make it a distributed database - need to add network support
        how to keep all databases consistent?  !!!consensus!!!
        Should all instances have the same internal format, or just the same keys/values?
        Would also need to timestamp stuff
